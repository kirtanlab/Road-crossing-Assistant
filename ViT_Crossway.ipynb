{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbfa882d",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1cfe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "2023-05-24 20:40:44.273860: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.9.16 (main, Mar  8 2023, 14:00:05) \\n[GCC 11.2.0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "import import_ipynb\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import *\n",
    "import glob\n",
    "import natsort\n",
    "import traceback\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ec4eac1",
   "metadata": {},
   "source": [
    "## Creating ViT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2c3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassToken(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
    "            trainable = True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        hidden_dim = self.w.shape[-1]\n",
    "\n",
    "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
    "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812b9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, cf):\n",
    "    print(cf)\n",
    "    x = Dense(3072, activation=\"gelu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(768)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    return x\n",
    "\n",
    "def transformer_encoder(x, cf):\n",
    "    skip_1 = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = MultiHeadAttention(\n",
    "        num_heads=12, key_dim=768\n",
    "    )(x, x)\n",
    "    x = Add()([x, skip_1])\n",
    "\n",
    "    skip_2 = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = mlp(x, cf)\n",
    "    x = Add()([x, skip_2])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a37858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViT(cf):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n",
    "    inputs = Input(input_shape)     ## (None, 256, 3072)\n",
    "\n",
    "    \"\"\" Patch + Position Embeddings \"\"\"\n",
    "    patch_embed = Dense(768)(inputs)   ## (None, 256, 768)\n",
    "\n",
    "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n",
    "    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=768)(positions) ## (256, 768)\n",
    "    embed = patch_embed + pos_embed ## (None, 256, 768)\n",
    "\n",
    "    \"\"\" Adding Class Token \"\"\"\n",
    "    token = ClassToken()(embed)\n",
    "    x = Concatenate(axis=1)([token, embed]) ## (None, 257, 768)\n",
    "\n",
    "    for _ in range(12):\n",
    "        x = transformer_encoder(x, cf)\n",
    "\n",
    "    \"\"\" Classification Head \"\"\"\n",
    "    x = LayerNormalization()(x)     ## (None, 257, 768)\n",
    "    x = x[:, 0, :]\n",
    "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deafdfa3",
   "metadata": {},
   "source": [
    "## HyperParams for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c78c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae3fb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Not all are used ,some are directly using values, change accordingly\n",
    "config = {}\n",
    "config[\"num_layers\"] = 12\n",
    "config[\"hidden_dim\"] = 768\n",
    "config[\"mlp_dim\"] = 3072\n",
    "config[\"num_heads\"] = 12\n",
    "config[\"dropout_rate\"] = 0.1\n",
    "config[\"num_patches\"] = 256\n",
    "config[\"patch_size\"] = 32\n",
    "config[\"num_channels\"] = 3\n",
    "config[\"num_classes\"] = 5\n",
    "\n",
    "# model = ViT(config)\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f683347",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d61bbf5e",
   "metadata": {},
   "source": [
    "### Setting Hyperparams for Dataset & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7816d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {}\n",
    "hp[\"image_size\"] = 200 #200*200\n",
    "\n",
    "hp[\"num_channels\"] = 3\n",
    "\n",
    "hp[\"patch_size\"] = 25 #25*25,set such as hp[\"image_size\"]%hp[\"patch_size\"] == 0\n",
    "\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\n",
    "\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"]) #(64,25*25*3)The input shape of Transformer\n",
    "\n",
    "hp[\"batch_size\"] = 16\n",
    "\n",
    "hp[\"lr\"] = 1e-4\n",
    "\n",
    "hp[\"num_epochs\"] = 100\n",
    "\n",
    "hp[\"num_classes\"] = 2\n",
    "# hp[\"class_names\"] = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159c6f8c",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489dee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 42\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbb31ddf",
   "metadata": {},
   "source": [
    "### Importing The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a402ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_videos = '/home/kirtan/Documents/FYProject/archive/Videos/Videos/'\n",
    "path_frames = '/home/kirtan/Documents/FYProject/archive/Videos/Frames/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c3d370",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e4a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57  40  36  17  67  35   8  44  69  70  28  20  85  26  74  50  14  25\n",
      "   4  18  39   9  82   7  68  37  91  84  55  51  71  47 100  62 101  97\n",
      "  42  59  49  90  58  76  33  98  60  64 102  38  30   2  53  22   3  24\n",
      "  88  95  75  87  83  21  61  72  15  93  52 103] 66\n",
      "[ 81  34  13  27  99  56  23  77  45  73  16  43  41  10  86  12 104  79\n",
      "  29  80   6  63] 22\n",
      "[31 66 65 54 46 94 92 48 11  1 19 32 89 96 78  5] 16\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1, 105)\n",
    "np.random.shuffle(x)\n",
    "videos_validation = x[:16]\n",
    "videos_test = x[16: 16+22]\n",
    "videos_train = x[16+22:]\n",
    "print(videos_train, len(videos_train))\n",
    "print(videos_test, len(videos_test))\n",
    "print(videos_validation, len(videos_validation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f78aa81",
   "metadata": {},
   "source": [
    "### Creating Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc59725",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_train = []\n",
    "labels_train = []\n",
    "filenames_validation = []\n",
    "labels_validation = []\n",
    "filenames_test = []\n",
    "labels_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b825e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 211, 151, 451, 391, 241, 331, 241, 211, 181, 241, 181, 181, 211, 331, 211, 181, 151, 211, 181, 151, 181, 241, 181, 211, 181, 181, 181, 181, 181, 211, 151, 181, 151, 211, 181, 181, 181, 181, 151, 211, 181, 181, 241, 181, 181, 181, 181, 181, 181, 241, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 211, 181, 181, 211, 181, 181, 181, 181, 181, 181, 181, 211, 181, 181, 181, 181, 181, 181, 211, 301, 481, 391, 151, 391, 571, 481, 481, 751, 781, 331, 571, 511, 451, 151, 571, 691, 421, 241, 331, 421, 421, 391, 301, 391]\n"
     ]
    }
   ],
   "source": [
    "colab_list =[104, 211, 151, 451, 391, 241, 331, 241, 211, 181, 241, 181, 181, 211, 331, 211, 181, 151, 211, 181, 151, 181, 241, 181, 211, 181, 181, 181, 181, 181, 211, 151, 181, 151, 211, 181, 181, 181, 181, 151, 211, 181, 181, 241, 181, 181, 181, 181, 181, 181, 241, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 211, 181, 181, 211, 181, 181, 181, 181, 181, 181, 181, 211, 181, 181, 181, 181, 181, 181, 211, 301, 481, 391, 151, 391, 571, 481, 481, 751, 781, 331, 571, 511, 451, 151, 571, 691, 421, 241, 331, 421, 421, 391, 301, 391]\n",
    "laptop_list=[]\n",
    "parent_dir = '/home/kirtan/Documents/FYProject/archive/Videos/Frames/'\n",
    "\n",
    "subdirectories = sorted([dirpath for dirpath, _, _ in os.walk(parent_dir)])\n",
    "\n",
    "for dirpath in subdirectories:\n",
    "    file_count = len(os.listdir(dirpath))\n",
    "    laptop_list.append(file_count)\n",
    "\n",
    "result = []\n",
    "\n",
    "for element1, element2 in zip(laptop_list, colab_list):\n",
    "    if element1 != element2:\n",
    "        result.append(-1)\n",
    "    else:\n",
    "        result.append(element1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2eb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in videos_train:\n",
    "    folder = path_frames + \"video{}/\".format(vid)\n",
    "    frames = glob.glob(folder + 'frame*.jpg')\n",
    "    frames = natsort.natsorted(frames)\n",
    "    filenames_train = np.append(filenames_train,frames)\n",
    "    labels_path = path_frames + \"video{}/\".format(vid) + \"labels{}.npy\".format(vid)\n",
    "    labels_array = np.load(labels_path)\n",
    "    labels_list = list(labels_array)\n",
    "    labels_train = np.append(labels_train,labels_list)\n",
    "\n",
    "filenames_train = np.array(filenames_train)\n",
    "labels_validation = np.array(labels_validation)\n",
    "\n",
    "for vid in videos_test:\n",
    "    folder = path_frames + \"video{}/\".format(vid)\n",
    "    frames = glob.glob(folder + 'frame*.jpg')\n",
    "    frames = natsort.natsorted(frames)\n",
    "    filenames_test = np.append(filenames_test,frames)\n",
    "    labels_path = path_frames + \"video{}/\".format(vid) + \"labels{}.npy\".format(vid)\n",
    "    labels_array = np.load(labels_path)\n",
    "    labels_list = list(labels_array)\n",
    "    labels_list = np.asarray(labels_list).astype('float32').reshape((-1,1))\n",
    "    labels_test = np.append(labels_test,labels_list)\n",
    "    \n",
    "filenames_test = np.array(filenames_test)\n",
    "labels_validation = np.array(labels_validation)\n",
    "\n",
    "for vid in videos_validation:\n",
    "    folder = path_frames + \"video{}/\".format(vid)\n",
    "    frames = glob.glob(folder + 'frame*.jpg')\n",
    "    frames = natsort.natsorted(frames)\n",
    "    filenames_validation = np.append(filenames_validation,frames)\n",
    "    labels_path = path_frames + \"video{}/\".format(vid) + \"labels{}.npy\".format(vid)\n",
    "    labels_array = np.load(labels_path)\n",
    "    labels_list = list(labels_array)\n",
    "    labels_list = np.asarray(labels_list).astype('float32').reshape((-1,1))\n",
    "    labels_validation = np.append(labels_validation,labels_list)\n",
    "\n",
    "filenames_validation = np.array(filenames_validation)\n",
    "labels_validation = np.array(labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3daf4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200,)\n"
     ]
    }
   ],
   "source": [
    "print(labels_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb66a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1880571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16890,) (4200,) (5430,)\n",
      "(16890,) (4200,) (5430,)\n"
     ]
    }
   ],
   "source": [
    "print(filenames_train.shape, filenames_validation.shape, filenames_test.shape)\n",
    "print(labels_train.shape, labels_validation.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e53d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kirtan/Documents/FYProject/archive/Videos/Frames/video57/frame0.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in filenames_train:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e876af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Had to create new folder named files\")\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9489cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38f6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    print('entering 1st')\n",
    "    image = cv2.imread(image_path.decode())\n",
    "    print('entering 2nd')\n",
    "\n",
    "    # Print image shape for debugging\n",
    "    print('Image shape:', image.shape)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32)\n",
    "    image = cv2.resize(image, (200, 200))\n",
    "    print('Image processed')\n",
    "\n",
    "    # Convert image to tf.Tensor\n",
    "    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "\n",
    "    # Apply random image transformations\n",
    "    image_tensor = tf.image.random_brightness(image_tensor, 0.15)\n",
    "    image_tensor = tf.image.random_contrast(image_tensor, 0.8, 1.5)\n",
    "    image_tensor = tf.image.random_saturation(image_tensor, 0.6, 3)\n",
    "    print('Image transformations applied')\n",
    "\n",
    "    # Convert image back to NumPy array\n",
    "    image_np = image_tensor.numpy()\n",
    "\n",
    "    patches = patchify(image_np, (25, 25, 3), 25)\n",
    "    patches = np.reshape(patches, (64, 25 * 25 * 3))\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    # Print patch shape for debugging\n",
    "    print('Patch shape:', patches.shape)\n",
    "\n",
    "    return patches, np.int32(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28d8ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path,labels):\n",
    "    print('ENTERING PARSE1ST')\n",
    "    patches,labels = tf.numpy_function(preprocess_image,[path,labels],[tf.float32,tf.int32])\n",
    "    print('ENTERING PARSE2ND')\n",
    "    print(patches.dtype)\n",
    "    patches.set_shape((64,25*25*3))\n",
    "    print('latest patches.shape,',patches.shape)\n",
    "    return patches,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c35787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images,labels,batch=16):\n",
    "    print(\"Running tf.dataset\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    print('Entering all 1st')\n",
    "    ds = ds.map(parse).batch(batch)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e4c4dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tf.dataset\n",
      "Entering all 1st\n",
      "ENTERING PARSE1ST\n",
      "ENTERING PARSE2ND\n",
      "<dtype: 'float32'>\n",
      "latest patches.shape, (64, 1875)\n",
      "Running tf.dataset\n",
      "Entering all 1st\n",
      "ENTERING PARSE1ST\n",
      "ENTERING PARSE2ND\n",
      "<dtype: 'float32'>\n",
      "latest patches.shape, (64, 1875)\n",
      "Running tf.dataset\n",
      "Entering all 1st\n",
      "ENTERING PARSE1ST\n",
      "ENTERING PARSE2ND\n",
      "<dtype: 'float32'>\n",
      "latest patches.shape, (64, 1875)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tf_dataset(filenames_train,labels_train)\n",
    "dataset_test = tf_dataset(filenames_test,labels_test)\n",
    "dataset_val = tf_dataset(filenames_validation,labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94fd1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"files\",\"model.h5\")\n",
    "csv_path = os.path.join(\"files\",\"log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6223cc40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n",
      "{'image_size': 200, 'num_channels': 3, 'patch_size': 25, 'num_patches': 64, 'flat_patches_shape': (64, 1875), 'batch_size': 16, 'lr': 0.0001, 'num_epochs': 100, 'num_classes': 2}\n"
     ]
    }
   ],
   "source": [
    "model = ViT(hp)\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
    "    metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a3d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "    CSVLogger(csv_path),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "debde632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     dataset_train,\n\u001b[1;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     validation_data\u001b[39m=\u001b[39;49mdataset_val,\n\u001b[1;32m      5\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/crossway/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/crossway/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=100,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c755c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
